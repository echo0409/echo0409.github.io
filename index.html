<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yake Wei</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/favicon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yake WEI/卫雅珂</name>
              </p>
              <p>I am a Ph.D student at <a href="https://gewu-lab.github.io/">GeWu-Lab</a>, <a href="http://ai.ruc.edu.cn/">Gaoling School of Artificial Intelligence</a>, Renmin University of China. I am advised by <a href="https://dtaoo.github.io/">Prof. Di Hu</a>. My research interests focus on multi-modal learning.
              </p>

              <p>I received my bachelor's degree in Computer Science and Technology from <a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China</a> (UESTC). Had a wondeful time with my friends in Chengdu, China from 2017-2021.
              </p>

              <p style="text-align:center">
                <a href="mailto:yakewei@ruc.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=i9mWGA0AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <!--
                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp  -->
                <a href="https://github.com/echo0409/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/IMG_1772.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/IMG_1772.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <strong>[2025-02]</strong> Awarded the Baidu Scholarship (10 Ph.D students worldwide)!
            </p>
            <p>
              <strong>[2024-12]</strong> Awarded the China National Scholarship for Ph.D student!
            </p>
            <p>
              <strong>[2024-12]</strong> Attended the Global PhD Gathering @ 2024 Pujiang AI Conference in Shanghai!
            </p>
            <p>
              <strong>[2024-11]</strong>  Gave a talk about "Balanced multimodal learning" @ Virginia Tech! Thanks the invitation from Prof. <a href="https://people.cs.vt.edu/chris/">Chris Thomas</a>!
            </p>
            <p>
              <strong>[2024-09]</strong>  One paper accepted by T-PAMI, thanks to all co-authors!
            </p>
            <p>
              <strong>[2024-07]</strong> Gave a talk about "Balanced multimodal learning" @ TechBeat! <a href="https://www.techbeat.net/talk-info?id=887">[Record]</a>
            </p>
            <p>
              <strong>[2024-07]</strong> One paper accepted by ECCV, thanks to all co-authors!
            </p>
            <p>
              <strong>[2024-05]</strong> We release a survey about fusion of low-quality multi-modal data! <a href="https://arxiv.org/abs/2404.18947">[arXiv]</a>
            </p>
            <p>
              <strong>[2024-05]</strong> One paper accepted by ICML, thanks to all co-authors!
            </p>
            <p>
              <strong>[2024-02]</strong> One paper accepted by CVPR, thanks to all co-authors!
            </p>
            <p>
              <strong>[2024-01]</strong> One paper accepted by ICLR, thanks to all co-authors!
            </p>
            <p>
              <strong>[2023-12]</strong> Start visiting in <a href="http://www.humansensing.cs.cmu.edu/">Human Sensing Lab</a> @ CMU!
            </p>
            <p>
              <strong>[2023-10]</strong> One paper accepted by Pattern Recognition, thanks to all co-authors!
            </p>
            <p>
              <strong>[2022-08]</strong> We release a survey about recent advances in audio-visual learning! <a href="https://gewu-lab.github.io/audio-visual-learning/">[website]</a>
            </p>
            <p>
              <strong>[2022-05]</strong> Gave a talk @ <a href="https://2022.baai.ac.cn/">2022 BAAI Conference</a> . Please find slides <a href="https://zenodo.org/record/6597495/files/2022%E6%99%BA%E6%BA%90%E5%A4%A7%E4%BC%9A-%E5%8D%AB%E9%9B%85%E7%8F%82.pdf?download=1">here</a>!
            </p>
            <p>
              <strong>[2022-03]</strong> Two papers accepted by CVPR, thanks to all co-authors!
            </p>
            <p>
              <strong>[2021-12]</strong> One paper accepted by T-PAMI, thanks to all co-authors!
            </p>
            <p>
              <strong>[2021-06]</strong> Graduate from University of Electronic Science and Technology of China (UESTC)!
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Selected Honors</heading>
          <p>
            &#x2022 Baidu Scholarship <strong>(10 Ph.D students worldwide) </strong>, 2025.
          </p>
          <p>
            &#x2022 China National Scholarship for Ph.D student <strong>(highest student honor in China, awarded to top 0.2%) </strong>, 2024.
          </p>
        </td>
      </tr>
    </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Services</heading>
          <p>
            <strong>Conference Reviewer:</strong> CVPR 2022-2024, ECCV 2022/2024, ICCV 2023, AAAI 2023-2025
          </p>
          <p>
            <strong>Journal Reviewer:</strong> TPAMI, TMM, TCSVT
          </p>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
      <heading style="padding:20px">Survey</heading>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/avlearning.jpg" alt="clean-usnob" width="320" height="160">
        </td>
        <td width="75%" valign="middle">
          <papertitle>Learning in Audio-visual Context: A Review, Analysis, and New Perspective</papertitle>
          <br>
          <br>
          <strong>Yake Wei</strong>, <a href="https://dtaoo.github.io">Di Hu</a>, <a href="https://yapengtian.org/">Yapeng Tian</a>, <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=zh-CN">Xuelong Li</a>
          <br>
          <br>

          <br>
          <a href="https://arxiv.org/abs/2208.09579">arXiv</a> / <a href="https://gewu-lab.github.io/audio-visual-learning/">website</a> / <a href="https://gewu-lab.github.io/awesome-audiovisual-learning/">awesome list</a> 
          <br>
          <p>A systematical survey about the audio-visual learning field.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/low_quality.png" alt="clean-usnob" width="320" height="160">
        </td>
        <td width="75%" valign="middle">
          <papertitle>Multimodal Fusion on Low-quality Data: A Comprehensive Survey</papertitle>
          <br>
          <br>
          Qingyang Zhang, <strong>Yake Wei</strong>, Zongbo Han, Huazhu Fu, Xi Peng, Cheng Deng, Qinghua Hu, Cai Xu, Jie Wen, <a href="https://dtaoo.github.io">Di Hu</a>, Changqing Zhang
          <br>
          <br>

          <br>
          <a href="https://arxiv.org/abs/2404.18947">arXiv</a> / <a href="https://github.com/QingyangZhang/awesome-low-quality-multimodal-learning">awesome list</a> 
          <br>
          <p>A systematical survey about fusion of low-quality multi-modal data.</p>
        </td>
      </tr>

    </tbody></table>

    <br>
    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading style="padding:20px">Publications</heading>(&#42; equal contribution)

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bimodulation.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>On-the-fly Modulation for Balanced Multimodal Learning</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, <a href="https://dtaoo.github.io">Di Hu</a>, Henghui Du, Ji-Rong Wen
              <br>
              P.S. Thanks the valuable help from Zequn Yang
              <br>
              <br>
              <em>T-PAMI</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2410.11582">arXiv</a> / <a href="https://github.com/GeWu-Lab/BML_TPAMI2024">code
              </a>
              <br>
              <p>Analyze and modulate imbalanced uni-modal learning from both feed-forward and back-propagation stage.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/reinit.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Diagnosing and Re-learning for Balanced Multimodal Learning</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, Siwei Li, Ruoxuan Feng, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2407.09705">arXiv</a> / <a href="https://github.com/GeWu-Lab/Diagnosing_Relearning_ECCV2024">code
              </a>
              <br>
              <p>Dynimically re-initialize uni-modal encoder to enhance both worse-learnt and well-learnt modalities.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mmpareto.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>ICML</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2405.17730">arXiv</a> / <a href="https://github.com/GeWu-Lab/MMPareto_ICML2024">code
              </a>
              <br>
              <p>Solve conflicts between multi-modal and uni-modal gradients under multi-modal scenarios.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sample-level.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Enhancing Multimodal Cooperation via Sample-level Modality Valuation</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, Ruoxuan Feng, Zihe Wang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2309.06255">arXiv</a> / <a href="https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation">code
              </a>
              <br>
              <p>Observe and improve the fine-grained cooperation between modalities at sample-level.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mmrobust.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Quantifying and Enhancing Multi-modal Robustness with Modality Preference</papertitle>
              <br>
              <br>
              Zequn Yang, <strong>Yake Wei</strong>, Ce Liang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>ICLR</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2402.06244.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/Certifiable-Robust-Multi-modal-Training">code
              </a>
              <br>
              <p>Analyze essential components for multi-modal robustness and delve into the
                limitations imposed by modality preference.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/geometric-mvc.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Geometric-inspired graph-based Incomplete Multi-view Clustering</papertitle>
              <br>
              <br>
              Zequn Yang, Han Zhang, <strong>Yake Wei</strong>, Zheng Wang, Feiping Nie, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>Pattern Recognition</em>, 2023
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S0031320323007793">paper</a> / <a href="https://github.com/GeWu-Lab/Geometric-Inspired-Graph-based-Incomplete-Multi-view-Clustering">code
              </a>
              <br>
              <p>Conduct geometric analyses to mitigate missing views in weight aggregation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/balance2022.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Balanced Multimodal Learning via On-the-fly Gradient Modulation</papertitle>
              <br>
              <br>
              Xiaokang Peng*, <strong>Yake Wei*</strong>, <a href="https://antony0621.github.io/">Andong Deng</a>, Dong Wang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/avqa2022.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Learning to Answer Questions in Dynamic Audio-Visual Scenarios</papertitle>
              <br>
              <br>
              <a href="https://ayameyao.github.io/">Guangyao Li*</a>, <strong>Yake Wei*</strong>, <a href="https://yapengtian.org/">Yapeng Tian*</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>, <a href="https://scholar.google.com/citations?user=tbxCHJgAAAAJ">Ji-Rong Wen</a>, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.14072.pdf">arXiv</a> / <a href="https://gewu-lab.github.io/MUSIC-AVQA/">project page
              </a>
              <br>
              <p>Audio-Visual Question Answering and propose MUSIC-AVQA dataset.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pami2021.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Class-aware Sounding Objects Localization via Audiovisual Correspondence</papertitle>
              <br>
              <br>
              <a href="https://dtaoo.github.io">Di Hu</a>, <strong>Yake Wei</strong>, <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>, <a href="https://weiyaolin.github.io/">Weiyao Lin</a>, <a href="https://scholar.google.com/citations?user=v5LctN8AAAAJ&hl">Ruihua Song</a>, <a href="https://scholar.google.com/citations?user=tbxCHJgAAAAJ">Ji-Rong Wen</a>
              <br>
              <br>
              <em>T-PAMI</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2112.11749.pdf">arXiv</a> / <a href="https://gewu-lab.github.io/CSOL_TPAMI2021/">project page
              </a>
              <br>
              <p>Discriminative sounding objects localization.</p>
            </td>
          </tr>

        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p font-size:small;>
                <br>
                <br>
                <div style="float:left;">
                    Updated at Feb. 2025
                </div>
                <div style="float:right;">
                    Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template.
                </div>
                <br>
                <br>        
            </p>                           
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
