<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yake Wei</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="src/favicon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yake WEI/卫雅珂</name>
              </p>
              <p>I am a Ph.D student at <a href="https://gewu-lab.github.io/">GeWu-Lab</a>, <a href="http://ai.ruc.edu.cn/">Gaoling School of Artificial Intelligence</a>, Renmin University of China. I am advised by <a href="https://dtaoo.github.io/">Prof. Di Hu</a>. My research interests focus on multimodal learning.
              </p>

              <p>I received my bachelor's degree in Computer Science and Technology from <a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China</a> (UESTC). Had a wondeful time with my friends in Chengdu, China from 2017-2021.
              </p>

              <p style="text-align:center">
                <a href="mailto:yakewei@ruc.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=i9mWGA0AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <!--
                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp  -->
                <a href="https://github.com/echo0409/">GitHub</a> &nbsp/&nbsp
                <a href="src/cv_yakewei_2025.pdf">CV</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="src/IMG_1772.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="src/IMG_1772.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <strong>[2025-06]</strong> Release our new PEFT pipeline for MLLMs, MokA! <a href="https://gewu-lab.github.io/MokA/">[Project page]</a>
            </p>
            <p>
              <strong>[2025-05]</strong> One paper accepted by ICML, thanks to all co-authors!
            </p>
            <p>
              <strong>[2025-03]</strong> Two paper accepted by CVPR, thanks to all co-authors!
            </p>
            <p>
              <strong>[2025-02]</strong> Awarded the <strong>Baidu Scholarship (10 Ph.D students worldwide)</strong>!
            </p>
            <p>
              <strong>[2024-12]</strong> Awarded the <strong>China National Scholarship for Ph.D student</strong>!
            </p>
            <p>
              <strong>[2024-12]</strong> Attended the Global PhD Gathering @ 2024 Pujiang AI Conference in Shanghai!
            </p>
            <p>
              <strong>[2024-11]</strong>  Gave a talk about "Balanced multimodal learning" @ Virginia Tech! Thanks the invitation from Prof. <a href="https://people.cs.vt.edu/chris/">Chris Thomas</a>!
            </p>
            <p>
              <strong>[2024-09]</strong>  One paper accepted by T-PAMI, thanks to all co-authors!
            </p>
            <p>
              <strong>[2024-07]</strong> Gave a talk about "Balanced multimodal learning" @ TechBeat! <a href="https://www.techbeat.net/talk-info?id=887">[Record]</a>
            </p>
            <p>
              <strong>[2024-07]</strong> One paper accepted by ECCV, thanks to all co-authors!
            </p>
            <p>
              <strong>[2024-05]</strong> We release a survey about fusion of low-quality multimodal data! <a href="https://arxiv.org/abs/2404.18947">[arXiv]</a>
            </p>
            <p>
              <strong>[2024-05]</strong> One paper accepted by ICML, thanks to all co-authors!
            </p>
            <p>
              <strong>[2024-02]</strong> One paper accepted by CVPR, thanks to all co-authors!
            </p>
            <p>
              <strong>[2024-01]</strong> One paper accepted by ICLR, thanks to all co-authors!
            </p>
            <p>
              <strong>[2023-12]</strong> Start visiting in <a href="http://www.humansensing.cs.cmu.edu/">Human Sensing Lab</a> @ CMU!
            </p>
            <p>
              <strong>[2023-10]</strong> One paper accepted by Pattern Recognition, thanks to all co-authors!
            </p>
            <p>
              <strong>[2022-08]</strong> We release a survey about recent advances in audio-visual learning! <a href="https://gewu-lab.github.io/audio-visual-learning/">[website]</a>
            </p>
            <p>
              <strong>[2022-05]</strong> Gave a talk @ <a href="https://2022.baai.ac.cn/">2022 BAAI Conference</a> . Please find slides <a href="https://zenodo.org/record/6597495/files/2022%E6%99%BA%E6%BA%90%E5%A4%A7%E4%BC%9A-%E5%8D%AB%E9%9B%85%E7%8F%82.pdf?download=1">here</a>!
            </p>
            <p>
              <strong>[2022-03]</strong> Two papers accepted by CVPR, thanks to all co-authors!
            </p>
            <p>
              <strong>[2021-12]</strong> One paper accepted by T-PAMI, thanks to all co-authors!
            </p>
            <p>
              <strong>[2021-06]</strong> Graduate from University of Electronic Science and Technology of China (UESTC)!
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Selected Honors</heading>
          <p>
            &#x2022 Baidu Scholarship <strong>(10 Ph.D students worldwide) </strong>, 2024.
          </p>
          <p>
            &#x2022 China National Scholarship for Ph.D student <strong>(highest student honor in China) </strong>, 2024.
          </p>
          <p>
            &#x2022 Outstanding Graduate of Sichuan province <strong>(highest honor for graduates set by Sichuan province)</strong>, 2021.
          </p>
          <p>
            &#x2022 Outstanding Graduate of University of Electronic Science and Technology of China, 2021.
          </p>
        </td>
      </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Research Highlights</heading>
        <p>
          Interested in the inherent learning mechanism of perceiving, formulating, and understanding the environment with heterogeneous information from multiple modalities, e.g., vision, sound, text.
        </p>    
        <p>
          In the paper presented at CVPR 2022 (ORAL), <strong>introduce the research topic of "Balanced Multimodal Learning" for the first time</strong>. Highlight a pervasive issue in multimodal learning, where information utilization of certain modality can be undesirably suppressed by others. </p>    
          <p>Then conduct a series of systematic studies to alleviate this issue, covering empirical observations, algorithms, and theoretical analysis.
          </p>
        <p>
          <strong>ONE MORE THING</strong>: We have a WeChat discussion group about balanced multimodal learning. Anyone who may be interested is welcome! <a href="https://github.com/GeWu-Lab/awesome-balanced-multimodal-learning">[Get QR Code here]</a>
        </p>
          <img src="src/my_work_2025.svg" alt="clean-usnob" width="700" vertical-align:middle horizontal-align:middle>
        


      </td>
    </tr>
  </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
      <heading style="padding:20px">Preprint</heading>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="src/moka.svg" alt="clean-usnob" width="320" height="160">
        </td>
        <td width="75%" valign="middle">
          <papertitle>MokA: Multimodal Low-Rank Adaptation for MLLMs</papertitle>
          <br>
          <br>
          <strong>Yake Wei</strong>, Yu Miao, Dongzhan Zhou, <a href="https://dtaoo.github.io">Di Hu</a>
          <br>
          <br>

          <br>
          <a href="https://gewu-lab.github.io/MokA/">Project page</a>
          <br>
          <p>A new PEFT pipeline for MLLMs, ensuring both unimodal and cross-modal adaptation.</p>
        </td>
      </tr>


    </tbody></table>

    <br>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
      <heading style="padding:20px">Survey</heading>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="src/avlearning.jpg" alt="clean-usnob" width="320" height="160">
        </td>
        <td width="75%" valign="middle">
          <papertitle>Learning in Audio-visual Context: A Review, Analysis, and New Perspective</papertitle>
          <br>
          <br>
          <strong>Yake Wei</strong>, <a href="https://dtaoo.github.io">Di Hu</a>, <a href="https://yapengtian.org/">Yapeng Tian</a>, <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=zh-CN">Xuelong Li</a>
          <br>
          <br>

          <br>
          <a href="https://arxiv.org/abs/2208.09579">arXiv</a> / <a href="https://gewu-lab.github.io/audio-visual-learning/">website</a> / <a href="https://gewu-lab.github.io/awesome-audiovisual-learning/">awesome list</a> 
          <br>
          <p>A systematical survey about the audio-visual learning field.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="src/low_quality.png" alt="clean-usnob" width="320" height="160">
        </td>
        <td width="75%" valign="middle">
          <papertitle>Multimodal Fusion on Low-quality Data: A Comprehensive Survey</papertitle>
          <br>
          <br>
          Qingyang Zhang, <strong>Yake Wei</strong>, Zongbo Han, Huazhu Fu, Xi Peng, Cheng Deng, Qinghua Hu, Cai Xu, Jie Wen, <a href="https://dtaoo.github.io">Di Hu</a>, Changqing Zhang
          <br>
          <br>

          <br>
          <a href="https://arxiv.org/abs/2404.18947">arXiv</a> / <a href="https://github.com/QingyangZhang/awesome-low-quality-multimodal-learning">awesome list</a> 
          <br>
          <p>A systematical survey about fusion of low-quality multimodal data.</p>
        </td>
      </tr>

    </tbody></table>

    <br>
    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading style="padding:20px">Publications (&#42; equal contribution)</heading>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/qrr.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Reviving the Cooperation Dynamics in Multimodal Transformer</papertitle>
              <br>
              <br>
              Haotian Ni, <strong>Yake Wei</strong>, Hang Liu, Gong Chen, Chong Peng, Hao Lin, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>ICML</em>, 2025
              <br>
              TBD
              <br>
              <p>Rebalance the attention score and revive cooperation dynamics between modalities in Transformer.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/iar.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Adaptive Unimodal Regulation for Balanced Multimodal Information Acquisition</papertitle>
              <br>
              <br>
              Chengxiang Huang*, <strong>Yake Wei*</strong>, Zequn Yang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2503.18595">arXiv</a> / <a href="https://github.com/GeWu-Lab/InfoReg_CVPR2025">code
              </a>
              <br>
              <p>Analyze and modulate information acquisition process during multimodal training process.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/dta.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Patch Matters: Training-free Fine-grained Image Caption Enhancement via Local Perception</papertitle>
              <br>
              <br>
              Ruotian Peng, Haiying He, <strong>Yake Wei</strong>, Yandong Wen, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2504.06666">arXiv</a> / <a href="https://github.com/GeWu-Lab/Patch-Matters">code
              </a>
              <br>
              <p>Generate high-quality fine-grained image caption by divide-then-aggregate strategy.</p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/bimodulation.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>On-the-fly Modulation for Balanced Multimodal Learning</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, <a href="https://dtaoo.github.io">Di Hu</a>, Henghui Du, Ji-Rong Wen
              <br>
              P.S. Thanks the valuable help from Zequn Yang
              <br>
              <br>
              <em>T-PAMI</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2410.11582">arXiv</a> / <a href="https://github.com/GeWu-Lab/BML_TPAMI2024">code
              </a>
              <br>
              <p>Analyze and modulate imbalanced unimodal learning from both feed-forward and back-propagation stage.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/acmmma.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Enhancing Modality Representation and Alignment for Multimodal Cold-start Active Learning</papertitle>
              <br>
              <br>
              Meng Shen, <strong>Yake Wei</strong>, Jianxiong Yin, Deepu Rajan, Di Hu, Simon See
              <br>
              <br>
              <em>ACM MM Asia</em>, 2024
              <br>
              <a href="https://dl.acm.org/doi/full/10.1145/3696409.3700225">paper</a>
              <br>
              <p>Improve the quality of selected multimodal data pairs in active learning.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/reinit.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Diagnosing and Re-learning for Balanced Multimodal Learning</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, Siwei Li, Ruoxuan Feng, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2407.09705">arXiv</a> / <a href="https://github.com/GeWu-Lab/Diagnosing_Relearning_ECCV2024">code
              </a>
              <br>
              <p>Dynimically re-initialize unimodal encoder to enhance both worse-learnt and well-learnt modalities.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/mmpareto.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>ICML</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2405.17730">arXiv</a> / <a href="https://github.com/GeWu-Lab/MMPareto_ICML2024">code
              </a>
              <br>
              <p>Solve conflicts between multimodal and unimodal gradients under multimodal scenarios.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/sample-level.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Enhancing Multimodal Cooperation via Sample-level Modality Valuation</papertitle>
              <br>
              <br>
              <strong>Yake Wei</strong>, Ruoxuan Feng, Zihe Wang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2309.06255">arXiv</a> / <a href="https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation">code
              </a>
              <br>
              <p>Observe and improve the fine-grained cooperation between modalities at sample-level.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/mmrobust.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Quantifying and Enhancing Multi-modal Robustness with Modality Preference</papertitle>
              <br>
              <br>
              Zequn Yang, <strong>Yake Wei</strong>, Ce Liang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>ICLR</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2402.06244.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/Certifiable-Robust-Multi-modal-Training">code
              </a>
              <br>
              <p>Analyze essential components for multimodal robustness and delve into the
                limitations imposed by modality preference.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/geometric-mvc.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Geometric-inspired graph-based Incomplete Multi-view Clustering</papertitle>
              <br>
              <br>
              Zequn Yang, Han Zhang, <strong>Yake Wei</strong>, Zheng Wang, Feiping Nie, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>Pattern Recognition</em>, 2023
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S0031320323007793">paper</a> / <a href="https://github.com/GeWu-Lab/Geometric-Inspired-Graph-based-Incomplete-Multi-view-Clustering">code
              </a>
              <br>
              <p>Conduct geometric analyses to mitigate missing views in weight aggregation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/balance2022.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Balanced Multimodal Learning via On-the-fly Gradient Modulation</papertitle>
              <br>
              <br>
              Xiaokang Peng*, <strong>Yake Wei*</strong>, <a href="https://antony0621.github.io/">Andong Deng</a>, Dong Wang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multimodal learning via on-the-fly gradient modulation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/avqa2022.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Learning to Answer Questions in Dynamic Audio-Visual Scenarios</papertitle>
              <br>
              <br>
              <a href="https://ayameyao.github.io/">Guangyao Li*</a>, <strong>Yake Wei*</strong>, <a href="https://yapengtian.org/">Yapeng Tian*</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>, <a href="https://scholar.google.com/citations?user=tbxCHJgAAAAJ">Ji-Rong Wen</a>, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.14072.pdf">arXiv</a> / <a href="https://gewu-lab.github.io/MUSIC-AVQA/">project page
              </a>
              <br>
              <p>Audio-Visual Question Answering and propose MUSIC-AVQA dataset.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="src/pami2021.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Class-aware Sounding Objects Localization via Audiovisual Correspondence</papertitle>
              <br>
              <br>
              <a href="https://dtaoo.github.io">Di Hu</a>, <strong>Yake Wei</strong>, <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>, <a href="https://weiyaolin.github.io/">Weiyao Lin</a>, <a href="https://scholar.google.com/citations?user=v5LctN8AAAAJ&hl">Ruihua Song</a>, <a href="https://scholar.google.com/citations?user=tbxCHJgAAAAJ">Ji-Rong Wen</a>
              <br>
              <br>
              <em>T-PAMI</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2112.11749.pdf">arXiv</a> / <a href="https://gewu-lab.github.io/CSOL_TPAMI2021/">project page
              </a>
              <br>
              <p>Discriminative sounding objects localization.</p>
            </td>
          </tr>

        </tbody></table>
        
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Services</heading>
            <p>
              <strong>Conference Reviewer:</strong> CVPR 2022-2025, ECCV 2022/2024, ICCV 2023, AAAI 2023-2025
            </p>
            <p>
              <strong>Journal Reviewer:</strong> T-PAMI, T-MM, T-CSVT
            </p>
          </td>
        </tr>
      </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p font-size:small;>
                <br>
                <br>
                <div style="float:left;">
                    Updated at June. 2025
                </div>
                <div style="float:right;">
                    Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template.
                </div>
                <br>
                <br>        
            </p>                           
            </td>
          </tr>
        </tbody></table>






      </td>
    </tr>
  </table>


  
</body>

</html>
